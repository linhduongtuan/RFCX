{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RFCX_train_from_npy_image_v3_6_.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "t3l_5_WXkHrd"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ITzFStSplR82"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MMoGy2fAk8Sa"
      },
      "source": [
        "%%time\n",
        "# 大体10分くらい\n",
        "\n",
        "!pip install -q kaggle\n",
        "!mkdir -p .kaggle\n",
        "!cp \"./drive/My Drive/Study/config/kaggle.json\" .kaggle/\n",
        "!chmod 600 .kaggle/kaggle.json\n",
        "!mv .kaggle /root\n",
        "\n",
        "!kaggle datasets download \"theoviel/rcfx-spectrograms-32-khz\"\n",
        "!unzip rcfx-spectrograms-32-khz.zip > /dev/null\n",
        "!rm -rf rcfx-spectrograms-32-khz.zip \n",
        "\n",
        "!pip install -U iterative-stratification albumentations wandb  > /dev/null\n",
        "!wandb login e0792bb688a0d18e359df7438c45da90f8794091\n",
        "\n",
        "!pip install timm\n",
        "!pip install imbalanced-learn"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3zAeTPFiliqi"
      },
      "source": [
        "import gc\n",
        "import os\n",
        "import tqdm\n",
        "import random\n",
        "import pickle\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "from iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n",
        "from datetime import datetime\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from numpy.random import beta\n",
        "\n",
        "import torch\n",
        "from torchvision.models import resnet18, densenet121\n",
        "import torch.nn as nn\n",
        "from torch.optim import Adam, AdamW\n",
        "from torch.optim.lr_scheduler import CosineAnnealingLR, ReduceLROnPlateau, CosineAnnealingWarmRestarts\n",
        "from torchvision import transforms\n",
        "from torch.nn import functional as F\n",
        "\n",
        "import albumentations as A\n",
        "\n",
        "from sklearn.metrics import classification_report, roc_auc_score\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "import wandb\n",
        "import timm\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "device = torch.device(\"cuda\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ISVFINs3TQz3"
      },
      "source": [
        "# timm.list_models(\"resnet*\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jxC7lTYBzFFM"
      },
      "source": [
        "N_EXP = \"0171\"\n",
        "#MODEL_NAME = \"resnet18\"\n",
        "MODEL_NAME = \"efficientnet_b0\"\n",
        "#MODEL_NAME = \"densenet121\"\n",
        "#MODEL_NAME = \"resnest50d\"\n",
        "DETAIL = \"pseudo_loss\"\n",
        "STAGE = \"4th\"  # 1st, 2nd, 3rd, 4th\n",
        "\n",
        "#BATCH_SZE = 64\n",
        "#BATCH_SZE = 32\n",
        "BATCH_SZE = 8\n",
        "#N_ACCUMULATE = 1\n",
        "N_ACCUMULATE = 4\n",
        "\n",
        "SEED = 416\n",
        "N_FOLD = 5\n",
        "WORKS = 0\n",
        "N_LABEL = 24\n",
        "\n",
        "EXP_NAME = f\"exp{N_EXP}_{MODEL_NAME}_{DETAIL}_{STAGE}\"\n",
        "OUTPUT = f\"./drive/MyDrive/Study/RFCX/output/{EXP_NAME}\"\n",
        "DATA_ROOT = \"./drive/MyDrive/Study/RFCX/input\"\n",
        "\n",
        "print(EXP_NAME)\n",
        "!mkdir -p {OUTPUT}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vf5dmlYmGIwT"
      },
      "source": [
        "データ読み込み"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HCDqE-MXlkkz"
      },
      "source": [
        "# Competition Data\n",
        "sample_submission = pd.read_csv(f\"{DATA_ROOT}/sample_submission.csv\")\n",
        "train_fp = pd.read_csv(f\"{DATA_ROOT}/train_fp.csv\")\n",
        "train_tp = pd.read_csv(f\"{DATA_ROOT}/train_tp.csv\")\n",
        "\n",
        "# Pseudo Label\n",
        "if STAGE == \"3rd\":\n",
        "    PSEUDO = \"exp0127_resnet18_second_st_only_label_loss_thr0.5\"\n",
        "    with open(f\"./drive/MyDrive/Study/RFCX/pseudo_labels/{PSEUDO}.pkl\", \"rb\") as f:\n",
        "        pseudo_label_dict_5fold = pickle.load(f)\n",
        "elif STAGE == \"4th\":\n",
        "    PSEUDO = \"exp0153_resnet18_focal_mixup_pseudo0.5_thr0.5\"\n",
        "    with open(f\"./drive/MyDrive/Study/RFCX/pseudo_labels/{PSEUDO}.pkl\", \"rb\") as f:\n",
        "        pseudo_label_dict_5fold = pickle.load(f)\n",
        "\n",
        "# Pre-Train Model\n",
        "if MODEL_NAME == \"resnet18\":\n",
        "    FIRST_ST = \"exp0109_only_clip_loss\"\n",
        "elif MODEL_NAME == \"densenet121\":\n",
        "    FIRST_ST = \"exp0157_densenet121_for_ensemble_1st\"\n",
        "elif MODEL_NAME == \"resnest50d\":\n",
        "    FIRST_ST = \"exp0117_resnest_now_best\"\n",
        "elif MODEL_NAME == \"efficientnet_b0\":\n",
        "    FIRST_ST = \"exp0158_efficientnet_b0_for_ensemble_1st\"\n",
        "\n",
        "# Parameter\n",
        "if STAGE == \"1st\":\n",
        "    LEARNING_RATE = 1e-3\n",
        "    NUM_EPOCHS = 50\n",
        "    T_MAX = 10\n",
        "else:\n",
        "    LEARNING_RATE = 3e-4\n",
        "    NUM_EPOCHS = 5\n",
        "    T_MAX = 5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HLeV-M8oGLir"
      },
      "source": [
        "utils"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kv4tt5lLl03a"
      },
      "source": [
        "# https://www.kaggle.com/c/rfcx-species-audio-detection/discussion/198418\n",
        "\n",
        "def LWLRAP(preds, labels):\n",
        "    ranked_classes = torch.argsort(preds, dim=-1, descending=True)\n",
        "    class_ranks = torch.zeros_like(ranked_classes)\n",
        "    for i in range(ranked_classes.size(0)):\n",
        "        for j in range(ranked_classes.size(1)):\n",
        "            class_ranks[i, ranked_classes[i][j]] = j + 1\n",
        "    ground_truth_ranks = class_ranks * labels + (1e6) * (1 - labels)\n",
        "    sorted_ground_truth_ranks, _ = torch.sort(ground_truth_ranks, dim=-1, descending=False)\n",
        "    num_labels = labels.sum(-1)\n",
        "    pos_matrix = torch.tensor(np.array([i+1 for i in range(labels.size(-1))])).unsqueeze(0)\n",
        "    score_matrix = pos_matrix / sorted_ground_truth_ranks\n",
        "    score_mask_matrix, _ = torch.sort(labels, dim=-1, descending=True)\n",
        "    scores = score_matrix * score_mask_matrix\n",
        "    score = scores.sum() / labels.sum()\n",
        "    return score.item()\n",
        "\n",
        "def set_seed(seed: int = 42):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)  # type: ignore\n",
        "    torch.backends.cudnn.deterministic = True  # type: ignore\n",
        "    torch.backends.cudnn.benchmark = True  # type: ignore"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4HRgn0P_W0CR"
      },
      "source": [
        "ラベルの抽出"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xwAUdP9jLuqS"
      },
      "source": [
        "tp_dict = {}\n",
        "for recording_id, df in train_tp.groupby(\"recording_id\"):\n",
        "    tp_dict[recording_id+\"_posi\"] = df.values[:, [1,3,4,5,6]]\n",
        "\n",
        "fp_dict = {}\n",
        "for recording_id, df in train_fp.groupby(\"recording_id\"):\n",
        "    fp_dict[recording_id+\"_nega\"] = df.values[:, [1,3,4,5,6]]\n",
        "\n",
        "tp_fnames, tp_labels = [], []\n",
        "for recording_id, df in train_tp.groupby(\"recording_id\"):\n",
        "    v = sum([np.eye(N_LABEL)[i] for i in df[\"species_id\"].tolist()])\n",
        "    v = (v  == 1).astype(int).tolist()\n",
        "    tp_fnames.append(recording_id+\"_posi\")\n",
        "    tp_labels.append(v)\n",
        "\n",
        "fp_fnames, fp_labels = [], []\n",
        "for recording_id, df in train_fp.groupby(\"recording_id\"):\n",
        "    v = sum([np.eye(N_LABEL)[i] for i in df[\"species_id\"].tolist()])\n",
        "    v = (v  == 1).astype(int).tolist()\n",
        "    fp_fnames.append(recording_id+\"_nega\")\n",
        "    fp_labels.append(v)\n",
        "\n",
        "test_fnames = sample_submission[\"recording_id\"].values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fwL1_s3YW_ga"
      },
      "source": [
        "DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bq4QHa4bGE3H"
      },
      "source": [
        "WINDOW = 512\n",
        "MAX_LEN = 3751\n",
        "SHIFT_W = [0, 32, 64, 128, 256]\n",
        "\n",
        "def extract_cut_pos(pos):\n",
        "\n",
        "    _shift_w = random.choice(SHIFT_W)\n",
        "    _shift_dist = random.choice([1, -1])\n",
        "    _pos = pos + (_shift_w * _shift_dist)\n",
        "\n",
        "    h_pos = _pos - 256\n",
        "    t_pos = _pos + 256\n",
        "    if h_pos <= 0:\n",
        "        h_pos, t_pos = 0, WINDOW\n",
        "    if t_pos >= MAX_LEN:\n",
        "        h_pos, t_pos = MAX_LEN-WINDOW, MAX_LEN\n",
        "    return h_pos, t_pos\n",
        "\n",
        "slide_img_pos = [[0, WINDOW]]\n",
        "for idx in range(1, 8):\n",
        "    h, t = slide_img_pos[idx-1][0], slide_img_pos[idx-1][1]\n",
        "    h = t - 49\n",
        "    t = h + WINDOW\n",
        "    slide_img_pos.append([h, t])\n",
        "\n",
        "print(slide_img_pos)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZzKRUeO9lqA4"
      },
      "source": [
        "class TimeMask:\n",
        "    def __init__(self, T=40, num_masks=1, replace_with_zero=True):\n",
        "        self.T = T\n",
        "        self.num_masks = num_masks\n",
        "        self.replace_with_zero = replace_with_zero\n",
        "\n",
        "    def __call__(self, spec):\n",
        "        cloned = spec.clone()\n",
        "        len_spectro = cloned.shape[2]\n",
        "    \n",
        "        for i in range(0, self.num_masks):\n",
        "            t = random.randrange(0, self.T)\n",
        "            t_zero = random.randrange(0, len_spectro - t)\n",
        "\n",
        "            # avoids randrange error if values are equal and range is empty\n",
        "            if (t_zero == t_zero + t): return cloned\n",
        "\n",
        "            mask_end = random.randrange(t_zero, t_zero + t)\n",
        "            if (self.replace_with_zero): cloned[:,:,t_zero:mask_end] = 0\n",
        "            else: cloned[:,:,t_zero:mask_end] = cloned.mean()\n",
        "        return cloned\n",
        "\n",
        "class FreqMask:\n",
        "    def __init__(self, F=30, num_masks=1, replace_with_zero=True):\n",
        "        self.F = F\n",
        "        self.num_masks = num_masks\n",
        "        self.replace_with_zero = replace_with_zero\n",
        "\n",
        "    def __call__(self, spec):\n",
        "        cloned = spec.clone()\n",
        "        num_mel_channels = cloned.shape[1]\n",
        "    \n",
        "        for i in range(0, self.num_masks):        \n",
        "            f = random.randrange(0, self.F)\n",
        "            f_zero = random.randrange(0, num_mel_channels - f)\n",
        "\n",
        "            # avoids randrange error if values are equal and range is empty\n",
        "            if (f_zero == f_zero + f): return cloned\n",
        "\n",
        "            mask_end = random.randrange(f_zero, f_zero + f) \n",
        "            if (self.replace_with_zero): cloned[:, f_zero:mask_end] = 0\n",
        "            else: cloned[:, f_zero:mask_end] = cloned.mean()\n",
        "    \n",
        "        return cloned\n",
        "\n",
        "def mono_to_color(\n",
        "    X: np.ndarray, mean=None, std=None,\n",
        "    norm_max=None, norm_min=None, eps=1e-6\n",
        "):\n",
        "    # Stack X as [X,X,X]\n",
        "    X = np.stack([X, X, X], axis=-1)\n",
        "\n",
        "    # Standardize\n",
        "    mean = mean or X.mean()\n",
        "    X = X - mean\n",
        "    std = std or X.std()\n",
        "    Xstd = X / (std + eps)\n",
        "    _min, _max = Xstd.min(), Xstd.max()\n",
        "    norm_max = norm_max or _max\n",
        "    norm_min = norm_min or _min\n",
        "    if (_max - _min) > eps:\n",
        "        # Normalize to [0, 255]\n",
        "        V = Xstd\n",
        "        V[V < norm_min] = norm_min\n",
        "        V[V > norm_max] = norm_max\n",
        "        V = 255 * (V - norm_min) / (norm_max - norm_min)\n",
        "        V = V.astype(np.uint8)\n",
        "    else:\n",
        "        # Just zero\n",
        "        V = np.zeros_like(Xstd, dtype=np.uint8)\n",
        "    return V\n",
        "\n",
        "def extract_seq_label(label, value):\n",
        "    seq_label = np.zeros((N_LABEL, 3751))\n",
        "    middle = np.ones(N_LABEL) * -1\n",
        "    for species_id, t_min, f_min, t_max, f_max in label:\n",
        "        h, t = int(3751*(t_min/60)), int(3751*(t_max/60))\n",
        "        m = (t + h)//2\n",
        "        middle[species_id] = m\n",
        "        seq_label[species_id, h:t] = value\n",
        "    return seq_label, middle.astype(int)\n",
        "\n",
        "class SpectrogramFromNpz(torch.utils.data.Dataset):\n",
        "    def __init__(self, fname, mode):\n",
        "        self.fname = fname\n",
        "        self.mode = mode\n",
        "        self.to_tensor = transforms.ToTensor()\n",
        "        self.norm = transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
        "        self.augument_funcs_a = A.Compose([\n",
        "            # A.RandomBrightnessContrast(p=0.5)\n",
        "        ])\n",
        "        self.augument_funcs_b = transforms.RandomApply([\n",
        "            #transforms.Lambda(lambda img: transforms.functional.adjust_gamma(img, gamma=2, gain=1)),\n",
        "            TimeMask(),\n",
        "            FreqMask(),\n",
        "        ], p=0.5)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.fname)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        fname = self.fname[idx]\n",
        "\n",
        "        # load label\n",
        "        if fname[-4:] == \"posi\":\n",
        "            label, middle = extract_seq_label(tp_dict[fname], 1)\n",
        "        elif fname[-4:] == \"nega\": \n",
        "            label, middle = extract_seq_label(fp_dict[fname], -1)\n",
        "        else:\n",
        "            label, middle = np.zeros((N_LABEL, 3751)), np.zeros(N_LABEL)\n",
        "\n",
        "        # load image\n",
        "        if self.mode in [\"train\", \"valid\"]:\n",
        "            if \"_posi\" in fname or \"_nega\" in fname:\n",
        "                img_dir = \"train\"\n",
        "            else:\n",
        "                img_dir = \"test\"\n",
        "        elif self.mode == \"test\":\n",
        "            img_dir = \"test\"\n",
        "        \n",
        "        _fname = fname.replace(\"_posi\", \"\").replace(\"_nega\", \"\")\n",
        "        path = f\"./{img_dir}/{_fname}.npy\"\n",
        "        mel = np.load(path)\n",
        "        image = mono_to_color(mel)\n",
        "\n",
        "        # augument\n",
        "        if self.mode == \"train\":\n",
        "            image = self.augument_funcs_a(image=image)[\"image\"]\n",
        "            image = self.to_tensor(image)\n",
        "            image = self.augument_funcs_b(image)\n",
        "        else:\n",
        "            image = self.to_tensor(image)\n",
        "        image = self.norm(image)\n",
        "\n",
        "        # pseudo\n",
        "        try:\n",
        "            pseudo_label = pseudo_label_dict_5fold[fname]\n",
        "        except KeyError:\n",
        "            pseudo_label = np.zeros((8, N_LABEL))\n",
        "        except NameError:\n",
        "            pseudo_label = np.zeros((8, N_LABEL))\n",
        "\n",
        "        return image, label, middle, pseudo_label"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TG2sqskrMPxh"
      },
      "source": [
        "#_datasets = SpectrogramFromNpz(tp_fnames[:2], \"train\")\n",
        "#_datasets = SpectrogramFromNpz(fp_fnames[:2], \"train\")\n",
        "#_datasets = SpectrogramFromNpz(test_fnames.tolist(), \"train\")\n",
        "#for d in _datasets:\n",
        "   #break\n",
        "   #pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I5azK91_Mpl_"
      },
      "source": [
        "#plt.imshow(d[0].transpose(0, 2).transpose(1, 0).numpy()); plt.show()\n",
        "#plt.plot(d[1].sum(0)); plt.show()\n",
        "#org_mage = mono_to_color(np.load(\"train/003bec244.npy\"))\n",
        "#plt.imshow(org_mage[:,2770:3282,:]); plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nDWcvxkKl70K"
      },
      "source": [
        "def rfcx_1st_criterion(outputs, targets, pseudo, b, perm):\n",
        "    clipwise_preds_att_ti = outputs[\"clipwise_preds_att_ti\"]\n",
        "    posi_label = ((targets == 1).sum(2) > 0).float().to(device)\n",
        "    posi_label = mixup(posi_label, b, perm)\n",
        "    loss = nn.BCEWithLogitsLoss(reduction=\"mean\")(clipwise_preds_att_ti, posi_label)\n",
        "    return loss\n",
        "\n",
        "def rfcx_2nd_criterion(outputs, targets, pseudo=None, b=None, perm=None):\n",
        "    clipwise_preds_att_ti = outputs[\"clipwise_preds_att_ti\"]\n",
        "    posi_label = ((targets == 1).sum(2) > 0).float().to(device)\n",
        "    nega_label = ((targets == -1).sum(2) > 0).float().to(device)\n",
        "    posi_y = torch.ones(clipwise_preds_att_ti.shape).to(device)\n",
        "    nega_y = torch.zeros(clipwise_preds_att_ti.shape).to(device)\n",
        "    posi_loss = nn.BCEWithLogitsLoss(reduction=\"none\")(clipwise_preds_att_ti, posi_y)\n",
        "    nega_loss = nn.BCEWithLogitsLoss(reduction=\"none\")(clipwise_preds_att_ti, nega_y)\n",
        "    posi_loss = (posi_loss * posi_label).sum()\n",
        "    nega_loss = (nega_loss * nega_label).sum()\n",
        "    loss = posi_loss + nega_loss\n",
        "    return loss\n",
        "\n",
        "def rfcx_3rd_criterion(outputs, targets, pseudo, b, perm):\n",
        "    clipwise_preds_att_ti = outputs[\"clipwise_preds_att_ti\"]\n",
        "\n",
        "    posi_pseudo = (pseudo > 0).int().to(device)\n",
        "    posi_label = ((targets == 1).sum(2) + posi_pseudo > 0).float().to(device)\n",
        "    nega_label = ((targets == -1).sum(2) > 0).float().to(device)\n",
        "\n",
        "    posi_y = torch.ones(clipwise_preds_att_ti.shape).to(device)\n",
        "    nega_y = torch.zeros(clipwise_preds_att_ti.shape).to(device)\n",
        "\n",
        "    # mixup treat\n",
        "    posi_label = mixup(posi_label, b, perm)\n",
        " \n",
        "    posi_loss = nn.BCEWithLogitsLoss(reduction=\"none\")(clipwise_preds_att_ti, posi_y)\n",
        "    nega_loss = nn.BCEWithLogitsLoss(reduction=\"none\")(clipwise_preds_att_ti, nega_y)\n",
        "\n",
        "    gamma = 2\n",
        "    probas = clipwise_preds_att_ti.sigmoid()\n",
        "    focal_pw = (1. - probas)**gamma\n",
        "\n",
        "    posi_loss = (posi_loss * posi_label * focal_pw).sum()\n",
        "    nega_loss = (nega_loss * nega_label).sum()\n",
        "\n",
        "    pos_w = 0 if posi_label.sum() == 0 else 1/posi_label.sum()\n",
        "    loss = posi_loss*pos_w + nega_loss\n",
        "\n",
        "    return loss\n",
        "\n",
        "def mixup(input, gamma, perm):\n",
        "    perm_input = input[perm]\n",
        "    return input.mul_(gamma).add_(1 - gamma, perm_input)\n",
        "\n",
        "def cut_and_sampling_img(X, y, m):\n",
        "    X_lst, y_lst = [], []\n",
        "    for _i, _m in enumerate(m):\n",
        "        idx = np.where(_m >= 0)\n",
        "        pos = _m[idx[0]]\n",
        "        pos = random.choice(pos)\n",
        "        h, t = extract_cut_pos(pos)\n",
        "        _X = X[_i, :, :, h:t]\n",
        "        _y = y[_i, :, h:t]\n",
        "        X_lst.append(_X)\n",
        "        y_lst.append(_y)\n",
        "    _X = torch.stack(X_lst)\n",
        "    _y = torch.stack(y_lst)\n",
        "    return _X, _y\n",
        "\n",
        "def split_and_padding(X, y):\n",
        "    x_lst = []\n",
        "    y_lst = []\n",
        "    for h, t in slide_img_pos:\n",
        "        _X = X[:, :, :, h:t]\n",
        "        _y = y[:, :, h:t]\n",
        "        if _X.shape[3] != WINDOW:\n",
        "            x_pad = torch.zeros(list(_X.shape[:-1]) + [WINDOW - _X.shape[3]])\n",
        "            _X = torch.cat([_X, x_pad], axis=3)\n",
        "            y_pad = torch.zeros(list(_y.shape[:-1]) + [WINDOW - _y.shape[2]])\n",
        "            _y = torch.cat([_y, y_pad], axis=2)\n",
        "        x_lst.append(_X)\n",
        "        y_lst.append(_y)\n",
        "    \n",
        "    X = torch.cat(x_lst, axis=0)\n",
        "    y = torch.cat(y_lst, axis=0)\n",
        "    return X, y\n",
        "\n",
        "def extract_pseudo_label(p):\n",
        "    p_lst = []\n",
        "    for i in range(8):\n",
        "        p_lst.append(p[:, i, :])\n",
        "    pseudo_y = torch.cat(p_lst, axis=0)\n",
        "    return pseudo_y\n",
        "\n",
        "def train_loop(train_data_loader, model, optimizer, scheduler, stage):\n",
        "    model.train()\n",
        "    losses, lrs = [], []\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    for n_iter, (X, y, m, p) in tqdm.tqdm_notebook(enumerate(train_data_loader), total=len(train_data_loader)):\n",
        "        if stage == \"1st\":\n",
        "            _X, _y = cut_and_sampling_img(X, y, m)\n",
        "        elif stage in [\"2nd\", \"3rd\", \"4th\"]:\n",
        "            _X, _y = split_and_padding(X, y)\n",
        "        _X, _y = _X.to(device), _y.to(device)\n",
        "\n",
        "        if stage in [\"3rd\", \"4th\"]:\n",
        "            pseudo_y = extract_pseudo_label(p)\n",
        "        elif stage in [\"1st\", \"2nd\"]:\n",
        "            pseudo_y = None\n",
        "\n",
        "        # mixup\n",
        "        b = beta(0.1, 0.1)\n",
        "        perm = torch.randperm(_X.size(0))\n",
        "        if stage in [\"1st\", \"3rd\"]:\n",
        "            _X = mixup(_X, b, perm)\n",
        "        \n",
        "        if stage == \"4th\":\n",
        "            outputs = model(_X, perm, b)\n",
        "        else:\n",
        "            outputs = model(_X)\n",
        "\n",
        "        loss = rfcx_criterion(outputs, _y, pseudo_y, b, perm)\n",
        "\n",
        "        loss.backward()\n",
        "        if n_iter % N_ACCUMULATE == 0:\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "        if scheduler is not None:\n",
        "            scheduler.step()\n",
        "\n",
        "        losses.append(loss.item())\n",
        "        lrs.append(np.array([param_group[\"lr\"] for param_group in optimizer.param_groups]).mean())\n",
        "    t_loss = np.array(losses).mean()\n",
        "    lr =  np.array(lrs).mean()\n",
        "    return t_loss, lr\n",
        "    \n",
        "def valid_loop(valid_data_loader, model, stage):\n",
        "    model.eval()\n",
        "    v_scores, v_losses = [], []\n",
        "    clip_preds, clip_targets = [], []\n",
        "    for X, y, m, p in valid_data_loader:\n",
        "        X = X.to(device)\n",
        "        if stage in [\"1st\", \"2nd\"]:\n",
        "            clip_y = ((y == 1).sum(2) > 0).float().to(device)\n",
        "        elif stage in [\"3rd\", \"4th\"]:\n",
        "            clip_y = ((y == 1).sum(2) + (p == 1).sum(1) > 0).float().to(device)\n",
        "        \n",
        "        preds = []\n",
        "        for h, t in slide_img_pos:\n",
        "            with torch.no_grad():\n",
        "                outputs = model(X[:,:,:,h:t])\n",
        "            pred, _ = outputs[\"segmentwise_output_ti\"].sigmoid().max(2)\n",
        "            preds.append(pred)\n",
        "        max_pred, _  = torch.max(torch.stack(preds), dim=0)\n",
        "            \n",
        "        score = LWLRAP(max_pred.cpu(), clip_y.cpu())\n",
        "        loss = nn.BCEWithLogitsLoss()(max_pred, clip_y).cpu().numpy()\n",
        "        v_scores.append(score)\n",
        "        v_losses.append(loss.item())\n",
        "\n",
        "        clip_preds.append(max_pred.cpu())\n",
        "        clip_targets.append(clip_y.cpu())\n",
        "\n",
        "    # calc precision\n",
        "    pred_y = torch.cat(clip_preds)\n",
        "    true_y = torch.cat(clip_targets)\n",
        "    lst = []\n",
        "    for _true_y, _pred_y in zip(true_y.T, pred_y.T):\n",
        "        res = classification_report(_true_y.int(), (_pred_y > 0.5).int(), output_dict=True)\n",
        "        res = res[\"1\"]\n",
        "        lst.append(res)\n",
        "    res_df = pd.DataFrame(lst)\n",
        "    precision_avg = res_df[\"precision\"].mean()\n",
        "\n",
        "    valid_score, valid_loss = np.array(v_scores).mean(), np.array(v_losses).mean()\n",
        "    return valid_score, valid_loss, precision_avg"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zhKxRdYVMQb-"
      },
      "source": [
        "# ヘッダーのインデックスと次元数\n",
        "MODEL_HEADER_INFO = {\n",
        "    \"resnet18\": (-2, 512),\n",
        "    \"densenet121\": (-2, 1024),\n",
        "    \"efficientnet_b0\": (-5, 320),\n",
        "    \"resnest50d\": (-2, 2048),\n",
        "    \"mobilenetv2_100\": (-2, 1280),\n",
        "}\n",
        "\n",
        "def interpolate(x: torch.Tensor, ratio: int):\n",
        "    x = x.transpose(1, 2)\n",
        "    (batch_size, time_steps, classes_num) = x.shape\n",
        "    upsampled = x[:, :, None, :].repeat(1, 1, ratio, 1)\n",
        "    upsampled = upsampled.reshape(batch_size, time_steps * ratio, classes_num)\n",
        "    upsampled = upsampled.transpose(1, 2)\n",
        "    return upsampled\n",
        "\n",
        "class RFCXNet(nn.Module):\n",
        "    def __init__(self, model_name):\n",
        "        super(RFCXNet, self).__init__()\n",
        "        self.model_name = model_name\n",
        "        self.n_label = N_LABEL\n",
        "\n",
        "        base_model = timm.create_model(model_name, pretrained=True)\n",
        "        h_idx, n_dense = MODEL_HEADER_INFO[model_name]        \n",
        "\n",
        "        # 過去学習に使ったモデルをロードするためヘッダーの名前を変える\n",
        "        if self.model_name in [\"resnet18\", \"efficientnet_b0\"]:\n",
        "            self.resnet_head = nn.Sequential(*list(base_model.children())[:h_idx])\n",
        "        elif self.model_name == \"resnest50d\":\n",
        "            self.resnest50d_head = nn.Sequential(*list(base_model.children())[:h_idx])\n",
        "        else:\n",
        "            self.model_head = nn.Sequential(*list(base_model.children())[:h_idx])\n",
        "                \n",
        "        self.fc_a = nn.Conv1d(n_dense, self.n_label, 1, bias=False)\n",
        "        self.fc_b = nn.Conv1d(n_dense, self.n_label, 1, bias=False)\n",
        "\n",
        "    def forward(self, x, perm=None, gamma=None):  # input x: (batch, channel, Hz, time)\n",
        "        frames_num = x.shape[3]\n",
        "        x = x.transpose(3, 2)  # (batch, channel, time, Hz)\n",
        "\n",
        "        # (batch, unit, time, Hz)\n",
        "        if self.model_name in [\"resnet18\", \"efficientnet_b0\"]:\n",
        "            h = self.resnet_head(x)  \n",
        "        elif self.model_name == \"resnest50d\":\n",
        "            h = self.resnest50d_head(x)\n",
        "        else:\n",
        "            h = self.model_head(x)\n",
        "        \n",
        "        if perm is not None:\n",
        "            h = gamma * h + (1 - gamma) * h[perm]\n",
        "            \n",
        "        h = F.relu(h)\n",
        "        ti_pool = torch.mean(h, dim=3)  # (batch, unit, time)\n",
        "\n",
        "        xa = self.fc_a(ti_pool)  # (batch, n_class, time)\n",
        "        xb = self.fc_b(ti_pool)  # (batch, n_class, time)\n",
        "        xb = torch.softmax(xb, dim=2)\n",
        "\n",
        "        # time pool\n",
        "        # pseudo_label_ti = ((xa.sigmoid() >= 0.5) > 0).float()\n",
        "        clipwise_preds_att_ti = torch.sum(xa * xb, dim=2)\n",
        "        # attention_preds_ti = xb\n",
        "        segmentwise_output_ti = interpolate(xa, 32)\n",
        "\n",
        "        return {\n",
        "            # \"pseudo_label_ti\": pseudo_label_ti,\n",
        "            \"clipwise_preds_att_ti\": clipwise_preds_att_ti,\n",
        "            # \"attention_preds_ti\": attention_preds_ti,\n",
        "            \"segmentwise_output_ti\": segmentwise_output_ti,\n",
        "        }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u7CpykTZPChS"
      },
      "source": [
        "#train_datasets = SpectrogramFromNpz(tp_fnames, \"train\")\n",
        "#train_data_loader = torch.utils.data.DataLoader(train_datasets, batch_size=2, shuffle=True, num_workers=0)\n",
        "#for n_iter, (X, y, m, p) in tqdm.tqdm_notebook(enumerate(train_data_loader)):\n",
        "#    _X, _y = split_and_padding(X, y)\n",
        "#    _X, _y = cut_and_sampling_img(X, y, m)\n",
        "#    preds = []\n",
        "#    for h, t in eval_img_pos:\n",
        "#        break\n",
        "    #break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kRHbWKj3Vwgd"
      },
      "source": [
        "#model = RFCXNet(\"resnet18\")\n",
        "#model.to(device)\n",
        "#model.eval()\n",
        "#with torch.no_grad():\n",
        "    #outputs = model(X[:,:,h:t].unsqueeze(0))\n",
        "    #outputs = model(_X.to(device))\n",
        "#outputs[\"segmentwise_output_ti\"].shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xzKg3IwYzKiH"
      },
      "source": [
        "# Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LzM8bj6PWfXm"
      },
      "source": [
        "mskf = MultilabelStratifiedKFold(n_splits=5, shuffle=True, random_state=SEED)\n",
        "test_datasets = SpectrogramFromNpz(test_fnames, \"test\")\n",
        "for fold, (train_index, valid_index) in enumerate(mskf.split(tp_fnames, tp_labels)):\n",
        "    if fold in [0]:\n",
        "        continue\n",
        "    print(datetime.now(), f\"\\t: ### FOLD-{fold} ###\")\n",
        "    set_seed(SEED+fold)\n",
        "\n",
        "    wandb.init(project=\"rfcx\", name=f\"{EXP_NAME}_f{fold}\")\n",
        "\n",
        "    config = wandb.config\n",
        "    config.exp_name = EXP_NAME\n",
        "    config.fold = fold\n",
        "    config.seed = SEED\n",
        "    config.learning_rate = LEARNING_RATE\n",
        "    config.batch_size = BATCH_SZE\n",
        "    config.num_epochs = NUM_EPOCHS\n",
        "    config.t_max = T_MAX\n",
        "    config.n_accumulate = N_ACCUMULATE\n",
        "\n",
        "    train_fname = np.array(tp_fnames)[train_index]\n",
        "    valid_fname = np.array(tp_fnames)[valid_index]\n",
        "\n",
        "    if STAGE == \"1st\":\n",
        "        train_datasets = SpectrogramFromNpz(train_fname.tolist()+fp_fnames[:30], \"train\")\n",
        "        rfcx_criterion = rfcx_1st_criterion\n",
        "    elif STAGE == \"2nd\":\n",
        "        train_datasets = SpectrogramFromNpz(train_fname.tolist()+fp_fnames, \"train\")\n",
        "        rfcx_criterion = rfcx_2nd_criterion\n",
        "    elif STAGE in [\"3rd\", \"4th\"]:\n",
        "        train_datasets = SpectrogramFromNpz(train_fname.tolist()+fp_fnames, \"train\")\n",
        "        rfcx_criterion = rfcx_3rd_criterion\n",
        "    \n",
        "    train_data_loader = torch.utils.data.DataLoader(train_datasets, batch_size=config.batch_size, shuffle=True, num_workers=WORKS)\n",
        "    valid_datasets = SpectrogramFromNpz(valid_fname, \"valid\")\n",
        "    valid_data_loader = torch.utils.data.DataLoader(valid_datasets, batch_size=config.batch_size, shuffle=False, num_workers=WORKS)\n",
        "\n",
        "    model = RFCXNet(MODEL_NAME)\n",
        "    if STAGE in [\"2nd\", \"3rd\", \"4th\"]:\n",
        "        model.load_state_dict(torch.load(f\"./drive/MyDrive/Study/RFCX/output/{FIRST_ST}/rfcxnet_f{config.fold}_best_score_model.bin\"))\n",
        "    model.to(device)\n",
        "\n",
        "    optimizer = Adam(model.parameters(), lr=config.learning_rate)\n",
        "    scheduler = CosineAnnealingLR(optimizer, T_max=len(train_data_loader)*config.t_max, eta_min=0.0)\n",
        "\n",
        "    wandb.watch(model)\n",
        "\n",
        "    print(datetime.now(), \"\\t: start train\")\n",
        "    best_score, best_loss, best_precision = 0, 9999, 0\n",
        "    for epoch in range(config.num_epochs):\n",
        "        if STAGE == \"1st\":\n",
        "            del train_datasets, train_data_loader\n",
        "            _fp_fnames = random.sample(fp_fnames, 30)\n",
        "            train_datasets = SpectrogramFromNpz(train_fname.tolist()+_fp_fnames, \"train\")\n",
        "            train_data_loader = torch.utils.data.DataLoader(train_datasets, batch_size=config.batch_size, shuffle=True, num_workers=WORKS)\n",
        "            gc.collect()\n",
        "\n",
        "        t_loss, lr = train_loop(train_data_loader, model, optimizer, scheduler, STAGE)\n",
        "        v_score, v_loss, precision_avg = valid_loop(valid_data_loader, model, STAGE)\n",
        "\n",
        "        if best_score < v_score:\n",
        "            print(f\"epoch {epoch}: best score update !!!\")\n",
        "            torch.save(model.state_dict(), f\"{OUTPUT}/rfcxnet_f{config.fold}_best_score_model.bin\")\n",
        "            best_score = v_score\n",
        "\n",
        "        if best_precision < precision_avg:\n",
        "            print(f\"epoch {epoch}: best precision update !!!\")\n",
        "            torch.save(model.state_dict(), f\"{OUTPUT}/rfcxnet_f{config.fold}_best_precision_model.bin\")\n",
        "            best_precision = precision_avg\n",
        "\n",
        "        if best_loss > v_loss:\n",
        "            print(f\"epoch {epoch}: best loss update !!!\")\n",
        "            torch.save(model.state_dict(), f\"{OUTPUT}/rfcxnet_f{config.fold}_best_loss_model.bin\")\n",
        "            best_loss = v_loss\n",
        "    \n",
        "        wandb.log({\"train loss\": t_loss, \"lr\": lr, \"valid loss\": v_loss, \"valid score\": v_score, \"best score\": best_score, \"best loss\": best_loss,\n",
        "                   \"precision\": precision_avg, \"best_precision\": best_precision})\n",
        "\n",
        "    print(datetime.now(), \"\\t: finish train\")\n",
        "    print(\"Best Valid Score:\", best_score)\n",
        "    wandb.finish()\n",
        "\n",
        "    # predict test data\n",
        "    model.load_state_dict(torch.load(f\"{OUTPUT}/rfcxnet_f{config.fold}_best_score_model.bin\"))\n",
        "    model.eval()\n",
        "\n",
        "    lst = []\n",
        "    for idx, (X, _, _, _) in tqdm.tqdm_notebook(enumerate(test_datasets), total=1992):\n",
        "        preds = []\n",
        "        for h, t in slide_img_pos:\n",
        "            with torch.no_grad():\n",
        "                outputs = model(X[:,:,h:t].unsqueeze(0).to(device))\n",
        "            pred, _ = outputs[\"segmentwise_output_ti\"].sigmoid().max(2)\n",
        "            preds.append(pred)\n",
        "        max_pred, _  = torch.max(torch.stack(preds), dim=0)\n",
        "        pred = max_pred.cpu().numpy()[0].tolist()\n",
        "\n",
        "        row = [test_fnames[idx]] + pred\n",
        "        lst.append(row)\n",
        "\n",
        "    fold_sub = pd.DataFrame(lst, columns=[\"recording_id\"]+[f\"s{i}\" for i in range(N_LABEL)])\n",
        "    fold_sub.to_csv(f\"{OUTPUT}/rfcxnet_f{config.fold}_predict.csv\", index=None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "txMDjtk8cNGv"
      },
      "source": [
        "# Test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bOR0_hcSoVC_"
      },
      "source": [
        "all_v_lst = []\n",
        "for fold in range(5):\n",
        "    df = pd.read_csv(f\"{OUTPUT}/rfcxnet_f{fold}_predict.csv\")\n",
        "    ids, v_lst = [], []\n",
        "    for row in df.values:\n",
        "        recording_id = row[0]\n",
        "        ids.append(recording_id)\n",
        "        v = torch.Tensor(row[1:].astype(float))\n",
        "        v_lst.append(v)\n",
        "    all_v_lst.append(torch.stack(v_lst, axis=0))\n",
        "\n",
        "all_preds = torch.stack(all_v_lst, axis=2).mean(2)\n",
        "sub = pd.DataFrame(all_preds.tolist(), columns=df.columns[1:])\n",
        "sub = pd.concat([df[[\"recording_id\"]], sub], axis=1)\n",
        "sub.to_csv(f\"./submission_{EXP_NAME}_avg.csv\", index=None)\n",
        "\n",
        "!cp \"./submission_{EXP_NAME}_avg.csv\" \"{OUTPUT}\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3cTIWwcG95ui"
      },
      "source": [
        "# Pseudo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gYtx2eJpHvGw"
      },
      "source": [
        "#EXP = \"exp0127_resnet18_second_st_only_label_loss\"\n",
        "EXP = \"exp0153_resnet18_focal_mixup_pseudo0.5\"\n",
        "# EXP = EXP_NAME\n",
        "PSEUDO_THR_P = 0.5\n",
        "PSEUDO_THR_N = 0.01\n",
        "BEST_MODEL = f\"./drive/MyDrive/Study/RFCX/output/{EXP}\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IuDPgQZV9ppY"
      },
      "source": [
        "model = RFCXNet(MODEL_NAME)\n",
        "model.to(device)\n",
        "all_fnames = tp_fnames+fp_fnames\n",
        "train_datasets = SpectrogramFromNpz(all_fnames, \"valid\")\n",
        "#test_datasets = SpectrogramFromNpz(test_fnames, \"test\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "duv3-RPNEI2B"
      },
      "source": [
        "pseudo_label_dict_lst = []\n",
        "for fold in range(5):\n",
        "    print(f\"### {fold} ###\")\n",
        "\n",
        "    model.load_state_dict(torch.load(f\"{BEST_MODEL}/rfcxnet_f{fold}_best_score_model.bin\"))\n",
        "    model.eval()\n",
        "\n",
        "    pseudo_label_dict = {}\n",
        "    for fname, (X, y, m, _) in tqdm.tqdm_notebook(zip(all_fnames, train_datasets), total=len(train_datasets)):\n",
        "    #for fname, (X, y, m, _) in tqdm.tqdm_notebook(zip(test_fnames, test_datasets), total=len(test_datasets)):\n",
        "        labels = []\n",
        "        posi_labels, nega_labels = [], []\n",
        "        for h, t in slide_img_pos:\n",
        "            with torch.no_grad():\n",
        "                outputs = model(X[:,:,h:t].unsqueeze(0).to(device))\n",
        "            pred = outputs[\"clipwise_preds_att_ti\"].sigmoid().cpu().numpy()[0]\n",
        "            \n",
        "            posi = (pred > PSEUDO_THR_P).astype(int)\n",
        "            nega = (pred < PSEUDO_THR_N).astype(int) * -1\n",
        "            posi_labels.append(posi)\n",
        "            nega_labels.append(nega)\n",
        "\n",
        "        posi_labels = np.stack(posi_labels).astype(int)\n",
        "        nega_labels = np.stack(nega_labels).astype(int)\n",
        "        if posi_labels.sum() == 0 and nega_labels.sum() == 0:\n",
        "            continue\n",
        "        labels = np.stack([posi_labels, nega_labels])\n",
        "        pseudo_label_dict[fname] = labels\n",
        "    pseudo_label_dict_lst.append(pseudo_label_dict)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P8OKfJTJ8dMN"
      },
      "source": [
        "pseudo_label_dict_5fold = {}\n",
        "for fname in all_fnames:\n",
        "#for fname in test_fnames:\n",
        "    labels = []\n",
        "    for pseudo_label_dict in pseudo_label_dict_lst:\n",
        "        try:\n",
        "            label = pseudo_label_dict[fname]\n",
        "        except KeyError:\n",
        "            label = np.zeros((2, 8, N_LABEL))\n",
        "        labels.append(label)\n",
        "\n",
        "    posi_label, nega_label = np.stack(labels).sum(0)\n",
        "    posi_label = (posi_label > 2).astype(int)\n",
        "    nega_label = (nega_label < -2).astype(int) * -1\n",
        "    new_label = posi_label + nega_label\n",
        "\n",
        "    if (new_label != 0).sum() == 0:\n",
        "        continue\n",
        "    pseudo_label_dict_5fold[fname] = new_label"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PF6Hp56Y98z0"
      },
      "source": [
        "lst = [(v==1).sum(0) > 0 for v in pseudo_label_dict_5fold.values()]\n",
        "pd.DataFrame(lst).sum(0).plot.bar()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uxGpSnaA-DaK"
      },
      "source": [
        "with open(f\"./drive/MyDrive/Study/RFCX/pseudo_labels/{EXP}_thr{PSEUDO_THR_P}.pkl\", \"wb\") as f:\n",
        "#with open(f\"./drive/MyDrive/Study/RFCX/pseudo_labels/{EXP}_test.pkl\", \"wb\") as f:\n",
        "    pickle.dump(pseudo_label_dict_5fold, f)\n",
        "    \n",
        "!ls ./drive/MyDrive/Study/RFCX/pseudo_labels/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w25yieceNZ1x"
      },
      "source": [
        "# emsemble"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WdVMwAVqNdqV"
      },
      "source": [
        "exp_lst = [\n",
        "    \"exp0167_resnet18_mixup_lastlayer_3rd\",\n",
        "    \"exp0166_densenet121_mixup_lastlayer_4th\",\n",
        "    \"exp0167_efficientnet_b0_mixup_lastlayer_3rd\",\n",
        "]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o9dRjVjpQNND"
      },
      "source": [
        "重み付け"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7fFHplUIOJVt"
      },
      "source": [
        "values = []\n",
        "for exp in exp_lst:\n",
        "    csv_path = f\"./drive/MyDrive/Study/RFCX/output/{exp}/submission_{exp}_avg.csv\"\n",
        "    df = pd.read_csv(csv_path)\n",
        "    values.append(df.values[:, 1:])\n",
        "\n",
        "values = np.stack(values)\n",
        "df[[f\"s{i}\" for i in range(24)]] = values.mean(0)\n",
        "df.to_csv(\"ensemble_resnet18_densenet121_efficientnet_b0_.csv\", index=None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Atrkuzr7QO_3"
      },
      "source": [
        "## Stacking"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9h6CbUImUrF0"
      },
      "source": [
        "def make_validation_prediction(valid_data_loader, model):\n",
        "    pred_y_lst, true_y_lst = [], []\n",
        "    for X, y, m, p in tqdm.tqdm_notebook(valid_data_loader):\n",
        "        X = X.to(device)\n",
        "        \n",
        "        # ラベルを予測\n",
        "        pred_y, true_y = [], []\n",
        "        for h, t in slide_img_pos:\n",
        "            with torch.no_grad():\n",
        "                outputs = model(X[:,:,:,h:t])\n",
        "            pred, _ = outputs[\"segmentwise_output_ti\"].sigmoid().max(2)\n",
        "            pred_y.append(pred)\n",
        "            t = ((y[:, :, h:t] > 0).sum(2) > 0).float()\n",
        "            true_y.append(t)\n",
        "        pred_y = torch.stack(pred_y).transpose(0, 1)\n",
        "        true_y = torch.stack(true_y).transpose(0, 1)\n",
        "\n",
        "        # pseudをつける\n",
        "        true_y = ((true_y + (p > 0).int()) > 0).int()\n",
        "\n",
        "        pred_y = pred_y.reshape((-1, 24))\n",
        "        true_y = true_y.reshape((-1, 24))\n",
        "\n",
        "        pred_y_lst.append(pred_y)\n",
        "        true_y_lst.append(true_y)\n",
        "    stacking_pred_y = torch.cat(pred_y_lst)\n",
        "    stacking_true_y = torch.cat(true_y_lst)\n",
        "    return stacking_pred_y, stacking_true_y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UbSBlVgWR3LN"
      },
      "source": [
        "def stacking_lr_model(train_x, train_y):\n",
        "    models = []\n",
        "    lst, scores = [], []\n",
        "    mskf_stacking = MultilabelStratifiedKFold(n_splits=5, shuffle=True, random_state=SEED)\n",
        "    for trn_i, val_i in mskf_stacking.split(train_x, train_y):\n",
        "        preds, targets = [], []\n",
        "        _models = []\n",
        "        for _id in range(24):\n",
        "            lr = LogisticRegression() \n",
        "            trn_x = train_x[trn_i, :] #[:, [0+_id, 24+_id, 48+_id]]\n",
        "            trn_y = train_y[trn_i, _id]\n",
        "            val_x = train_x[val_i, :] #[:, [0+_id, 24+_id, 48+_id]]\n",
        "            val_y = train_y[val_i, _id]\n",
        "            lr.fit(trn_x, trn_y)\n",
        "            val_pred = lr.predict_proba(val_x)[:, 1]\n",
        "            preds.append(val_pred)\n",
        "            targets.append(val_y)\n",
        "            res = classification_report(val_y, (val_pred>0.5).astype(int), output_dict=True)[\"1\"]\n",
        "            lst.append(res)\n",
        "            _models.append(lr)\n",
        "        preds = np.vstack(preds).reshape((-1, 24))\n",
        "        targets = np.vstack(targets).reshape((-1, 24))\n",
        "        score = LWLRAP(torch.tensor(preds), torch.tensor(targets))\n",
        "        scores.append(score)\n",
        "        models.append(_models)\n",
        "    results = pd.DataFrame(lst).mean(0)\n",
        "    results[\"LWLRAP\"] = sum(scores)/5\n",
        "    return models, results"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZlxDZokwOM7s"
      },
      "source": [
        "test_preds, results_lst = [], []\n",
        "mskf = MultilabelStratifiedKFold(n_splits=5, shuffle=True, random_state=SEED)\n",
        "for fold, (train_index, valid_index) in enumerate(mskf.split(tp_fnames, tp_labels)):\n",
        "    print(f\"### Fold-{fold} ###\")\n",
        "    valid_fname = np.array(tp_fnames)[valid_index]\n",
        "    valid_datasets = SpectrogramFromNpz(valid_fname, \"valid\")\n",
        "    valid_data_loader = torch.utils.data.DataLoader(valid_datasets, batch_size=32, shuffle=False, num_workers=WORKS)\n",
        "\n",
        "    stacking_pred_y_lst = []\n",
        "    values = []\n",
        "    for exp in exp_lst:\n",
        "        model_name = exp.split(\"_\")[1]\n",
        "        if \"efficientnet\" in model_name:\n",
        "            model_name += \"_\" + exp.split(\"_\")[2]\n",
        "        print(model_name)\n",
        "        model = RFCXNet(model_name)\n",
        "        model.to(device)\n",
        "        model.load_state_dict(torch.load(f\"./drive/MyDrive/Study/RFCX/output/{exp}/rfcxnet_f{fold}_best_score_model.bin\"))\n",
        "        model.eval()\n",
        "        stacking_pred_y, stacking_true_y = make_validation_prediction(valid_data_loader, model)\n",
        "        stacking_pred_y_lst.append(stacking_pred_y)\n",
        "\n",
        "        csv_path = f\"./drive/MyDrive/Study/RFCX/output/{exp}/submission_{exp}_avg.csv\"\n",
        "        test_df = pd.read_csv(csv_path)\n",
        "        values.append(test_df.values[:, 1:])\n",
        "\n",
        "    train_x = torch.hstack(stacking_pred_y_lst).cpu().numpy()\n",
        "    train_y = stacking_true_y.numpy()\n",
        "    models, results = stacking_lr_model(train_x, train_y)\n",
        "    display(results)\n",
        "    results_lst.append(results)\n",
        "\n",
        "    for _models in models:\n",
        "        preds = []\n",
        "        for _id, m in enumerate(_models):\n",
        "            _pred = m.predict_proba(np.hstack(values))[:, 1]\n",
        "            preds.append(_pred)\n",
        "        test_preds.append(np.stack(preds).T)\n",
        "\n",
        "test_preds_avg = np.stack(test_preds).mean(0)\n",
        "sum(results_lst)/5\n",
        "\n",
        "test_df[[f\"s{i}\" for i in range(24)]] = test_preds_avg\n",
        "test_df.to_csv(f\"ensemble_stacking.csv\", index=None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4xKFh4ljstkU"
      },
      "source": [
        "for exp in exp_lst:\n",
        "    csv_path = f\"./drive/MyDrive/Study/RFCX/output/{exp}/submission_{exp}_avg.csv\"\n",
        "    df1 = pd.read_csv(csv_path)\n",
        "    break\n",
        "df2 = pd.read_csv(\"ensemble_resnet18_densenet121_efficientnet_b0_.csv\")\n",
        "df3 = pd.read_csv(\"ensemble_stacking.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gdfR1rgJtKmZ"
      },
      "source": [
        "for recording_id in df1[\"recording_id\"].iloc[1:]:\n",
        "    v1 = df1.query(f\"recording_id=='{recording_id}'\").values[0, 1:]\n",
        "    v2 = df2.query(f\"recording_id=='{recording_id}'\").values[0, 1:]\n",
        "    v3 = df3.query(f\"recording_id=='{recording_id}'\").values[0, 1:]\n",
        "    break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BpmAF279tNO1"
      },
      "source": [
        "plt.plot(v1)\n",
        "plt.plot(v2)\n",
        "plt.plot(v3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Igerf6DttT26"
      },
      "source": [
        "np.argsort(v1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Rgvug_LuJGW"
      },
      "source": [
        "np.argsort(v2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E9MUWKHiuKGf"
      },
      "source": [
        "np.argsort(v3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Duzw8gSpuK7T"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}